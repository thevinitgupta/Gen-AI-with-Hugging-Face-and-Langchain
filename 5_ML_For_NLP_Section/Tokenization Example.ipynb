{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab32438-70fb-4842-b5eb-ffcd31554cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/genai-basics/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/genai-basics/lib/python3.13/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/genai-basics/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/genai-basics/lib/python3.13/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/genai-basics/lib/python3.13/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a590f3-aa71-491a-9022-49bc01add1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"\n",
    "Introduction to Tokenization Practicals\n",
    "In this session, we continue our discussion on Natural Language Processing (NLP) by focusing on practical aspects of tokenization. \n",
    "Previously, we explored basic terminologies such as corpus, paragraph, vocabulary, and words. Now, we will proceed to hands-on implementation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f7f616-8065-4983-ac86-a5f7f3549b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Introduction to Tokenization Practicals\n",
      "In this session, we continue our discussion on Natural Language Processing (NLP) by focusing on practical aspects of tokenization. \n",
      "Previously, we explored basic terminologies such as corpus, paragraph, vocabulary, and words. Now, we will proceed to hands-on implementation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d5c7ca-97a6-42a0-9d43-8413972cce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/thevinitgupta/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "037da653-0529-4566-9f83-726c0a3e8d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nIntroduction to Tokenization Practicals\\nIn this session, we continue our discussion on Natural Language Processing (NLP) by focusing on practical aspects of tokenization.', 'Previously, we explored basic terminologies such as corpus, paragraph, vocabulary, and words.', 'Now, we will proceed to hands-on implementation.']\n"
     ]
    }
   ],
   "source": [
    "# Convert para to sentences\n",
    "documents=nltk.tokenize.sent_tokenize(corpus)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "293dab5f-e89e-4486-a196-f01ad9858be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67c356c8-d435-4d2a-b821-4b4a7dea6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3ca0cd-9444-4694-953c-2b3448a06e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction',\n",
       " 'to',\n",
       " 'Tokenization',\n",
       " 'Practicals',\n",
       " 'In',\n",
       " 'this',\n",
       " 'session',\n",
       " ',',\n",
       " 'we',\n",
       " 'continue',\n",
       " 'our',\n",
       " 'discussion',\n",
       " 'on',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'by',\n",
       " 'focusing',\n",
       " 'on',\n",
       " 'practical',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'tokenization',\n",
       " '.',\n",
       " 'Previously',\n",
       " ',',\n",
       " 'we',\n",
       " 'explored',\n",
       " 'basic',\n",
       " 'terminologies',\n",
       " 'such',\n",
       " 'as',\n",
       " 'corpus',\n",
       " ',',\n",
       " 'paragraph',\n",
       " ',',\n",
       " 'vocabulary',\n",
       " ',',\n",
       " 'and',\n",
       " 'words',\n",
       " '.',\n",
       " 'Now',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'proceed',\n",
       " 'to',\n",
       " 'hands-on',\n",
       " 'implementation',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "122c866a-176a-4bf4-a23f-431f98ba9da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction',\n",
       " 'to',\n",
       " 'Tokenization',\n",
       " 'Practicals',\n",
       " 'In',\n",
       " 'this',\n",
       " 'session',\n",
       " ',',\n",
       " 'we',\n",
       " 'continue',\n",
       " 'our',\n",
       " 'discussion',\n",
       " 'on',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'by',\n",
       " 'focusing',\n",
       " 'on',\n",
       " 'practical',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'tokenization',\n",
       " '.',\n",
       " 'Previously',\n",
       " ',',\n",
       " 'we',\n",
       " 'explored',\n",
       " 'basic',\n",
       " 'terminologies',\n",
       " 'such',\n",
       " 'as',\n",
       " 'corpus',\n",
       " ',',\n",
       " 'paragraph',\n",
       " ',',\n",
       " 'vocabulary',\n",
       " ',',\n",
       " 'and',\n",
       " 'words',\n",
       " '.',\n",
       " 'Now',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'proceed',\n",
       " 'to',\n",
       " 'hands',\n",
       " '-',\n",
       " 'on',\n",
       " 'implementation',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893039bd-0438-44ab-accc-69e910bc1643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8e687-abe9-4c73-ab5b-a05f15bf5b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c58b51-798c-447c-84f1-9df47f4c022b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
